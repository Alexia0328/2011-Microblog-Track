{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTzHLN_XlddN",
    "outputId": "6b7d7539-5b8a-45b7-9707-de823decb39e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt \n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import numpy as np\n",
    "from queue import PriorityQueue\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFc2GTv2qB8F",
    "outputId": "e10e2556-967a-435a-98ac-a56a5d10e218"
   },
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    file=open(\"Trec_microblog11.txt\",encoding='utf-8')\n",
    "    line=file.readline()\n",
    "    nums=[]\n",
    "    docs=[]\n",
    "    while line:\n",
    "      a = line.split('\\t')\n",
    "      num = a[0:1]\n",
    "      doc = a[1:]\n",
    "      nums.append(num)\n",
    "      docs.append(doc)\n",
    "      line = file.readline()\n",
    "    return nums,docs\n",
    "\n",
    "def train_docs():\n",
    "  newdocs = []\n",
    "  for doc in docs:\n",
    "    doc[0] = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', \" \", doc[0])\n",
    "    doc[0] = re.sub(r'@([a-z]+) ([a-z]+),',' ', doc[0]) \n",
    "    punc = '~`!#$%^&*()_+-=|\\';\":/.,?><~·！@#￥%……&*（）——+-=“：’;.，?》《><{}``'\n",
    "    doc[0] = re.sub(r\"[%s]+\" %punc, \" \",doc[0]) \n",
    "    doc[0] = doc[0].replace('\\t',' ')\n",
    "    doc[0] = doc[0].replace('\\xa0',' ')\n",
    "    newdoc = doc[0].replace('\\n','')\n",
    "    newdoc = doc[0].lower() \n",
    "    newdocs.append(text_processing(newdoc))\n",
    "  return newdocs\n",
    "\n",
    "def text_processing(question):\n",
    "    stopWords=set(stopwords.words('english'))\n",
    "    every_question_word = list()\n",
    "    sentence = []\n",
    "    for word in word_tokenize(question):\n",
    "      if (word not in stopWords):\n",
    "        every_question_word.append(lemmatizer.lemmatize(word))\n",
    "    return every_question_word\n",
    "\n",
    "nums,docs = get_dataset()\n",
    "newdocs = train_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cKtxKm0dT5p9"
   },
   "outputs": [],
   "source": [
    "question_list = newdocs\n",
    "word_in_q = {}\n",
    "\n",
    "for x in question_list:\n",
    "  for y in x:\n",
    "    if y in word_in_q:\n",
    "      word_in_q[y] +=1\n",
    "    else:\n",
    "      word_in_q[y] = 1\n",
    "\n",
    "word_num = sum(word_in_q.values())\n",
    "word_different = len(word_in_q.keys())\n",
    "\n",
    "dictionary = dict(sorted(word_in_q.items(),key=lambda x:x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dpXECbHBUUG5",
    "outputId": "66a172b6-dcc6-4287-c8e5-01015b29c1c1"
   },
   "outputs": [],
   "source": [
    "# it will take long time like 3-5min, please wait for it\n",
    "all_question_word = question_list\n",
    "index_inverted = dict()\n",
    "for question in question_list:\n",
    "    for word in question:\n",
    "        if word not in index_inverted:\n",
    "            index_inverted.update({word: [question_list.index(question)]})\n",
    "        else:\n",
    "            index_inverted[word].append(question_list.index(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VqAJmK6VbYnd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-f86ad2a24cba>:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  model.train(all_question_word,total_examples=model.corpus_count, epochs=model.iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17054520, 17369170)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it will take long time like 3-5min, please wait for it\n",
    "model = Word2Vec(all_question_word,min_count=1,size = 200,iter = 30,sample=1e-3,sg=1,window = 5)\n",
    "model.save(\"word2vec.model\")\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "model.train(all_question_word,total_examples=model.corpus_count, epochs=model.iter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ewc2IJJ8b0qY"
   },
   "outputs": [],
   "source": [
    "def get_query():\n",
    "    f=open(\"topics_MB1-49.txt\",encoding='utf-8')\n",
    "    line=f.readline()\n",
    "    query=[]\n",
    "    while line:\n",
    "        thisquery=[]\n",
    "        if ('<title>' in line):\n",
    "          line=line.strip('<title>')\n",
    "          line=line.strip('</title>\\n')\n",
    "          line = re.sub('\",', \" \", line)\n",
    "          line = line.lower() \n",
    "          words=word_tokenize(line)\n",
    "          for i in words:\n",
    "            if i != \"''\" and i !='``' and i !=\"'\" and i !=',':\n",
    "              thisquery.append(i)\n",
    "          query.append(thisquery)\n",
    "        else:\n",
    "          line = f.readline()\n",
    "    return query\n",
    "queries = get_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUz2YlXKfAYT",
    "outputId": "22f00892-9c0a-48c1-ae78-813ac48360dd"
   },
   "outputs": [],
   "source": [
    "findnumslist = []\n",
    "percs = []\n",
    "from scipy import spatial\n",
    "\n",
    "for real_question_word in queries:\n",
    "  filter_index = set()\n",
    "  real_question_word_exist = real_question_word[:] \n",
    "  for word in real_question_word:   \n",
    "    if word not in index_inverted.keys():\n",
    "      real_question_word_exist.remove(word)\n",
    "    else:\n",
    "      filter_index.update(index_inverted[word])\n",
    "  part_question_word = list()\n",
    "  for index in filter_index:\n",
    "    one_question_word = all_question_word[index]\n",
    "    part_question_word.append(one_question_word)\n",
    "  siml_list = list()\n",
    "  for question_word in part_question_word:\n",
    "    siml=model.wv.n_similarity(real_question_word_exist,question_word)\n",
    "    siml_list.append(siml)\n",
    "  if len(siml_list)==0:\n",
    "    findnumslist.append([])\n",
    "    percs.append([])\n",
    "  elif max(siml_list)>0:\n",
    "    top_question = []\n",
    "    for i in range(len(siml_list)):\n",
    "        top_question.append((siml_list[i],part_question_word[i]))\n",
    "    sorted_top_question = sorted(top_question)\n",
    "    sorted_top_question.reverse()\n",
    "    if len(sorted_top_question) > 1000:\n",
    "        sorted_top_question = sorted_top_question[:1000] \n",
    "    findnums = []\n",
    "    perc = []\n",
    "    for top in sorted_top_question:\n",
    "      per,find = top\n",
    "      findnum = nums[newdocs.index(find)]\n",
    "      findnums.append(findnum)\n",
    "      perc.append(per)\n",
    "\n",
    "    findnumslist.append(findnums)\n",
    "    percs.append(perc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7DHk0grKlAOB"
   },
   "outputs": [],
   "source": [
    "def output():\n",
    "  with open('outputword2vec.txt','w') as fa:\n",
    "    for i in range(len(findnumslist)):\n",
    "      if findnumslist[i] != []:\n",
    "        for j in range(len(findnumslist[i])):\n",
    "          out = str(i+1) + ' Q0 ' + str(findnumslist[i][j][0]) + ' ' + str(j+1) + ' ' + str(percs[i][j]) + ' query NO.' + str(j+1) + ''\"\\n\"\n",
    "          fa.write(out)\n",
    "    fa.close()\n",
    "output()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "try.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
